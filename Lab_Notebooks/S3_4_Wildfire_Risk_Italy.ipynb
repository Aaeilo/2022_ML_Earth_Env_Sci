{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S3_4_Wildfire_Risk_Italy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPi8g/B0ZoGuK5TpBj4wgCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S3_4_Wildfire_Risk_Italy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This"
      ],
      "metadata": {
        "id": "bstlA2R4LZcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Credits**\n",
        "\n",
        "This online tutorial would not be possible without invaluable contributions from [Andrea Trucchia](https://www.researchgate.net/profile/Andrea-Trucchia) (reduced data, methods), [Giorgio Meschi](https://www.linkedin.com/in/giorgio-meschi-86216b180/) (code, methods), and [Marj Tonini](https://www.researchgate.net/profile/Marj-Tonini-2) (presentation, methods). It builds upon the following article:\n",
        "\n",
        "[Trucchia, A.; Meschi, G.; Fiorucci, P.; Gollini, A.; Negro, D., Defining Wildfire Susceptibility Maps in Italy for Understanding Seasonal Wildfire Regimes at the National Level, *Fire*, (2022)](https://www.mdpi.com/2571-6255/5/1/30) \n",
        "\n",
        "which generalizes the study below from the Liguria region to all of Italy:\n",
        "\n",
        "[Tonini, Marj, et al. \"A machine learning-based approach for wildfire susceptibility mapping. The case study of the Liguria region in Italy.\" *Geosciences* 10.3 (2020): 105.](https://www.mdpi.com/2076-3263/10/3/105)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BaCACOGXLhCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik8F8GJUK1OW"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Feb 17 11:38:08 2022\n",
        "\n",
        "@author: Giorg\n",
        "\"\"\"\n",
        "#%% funtions\n",
        "\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "\n",
        "# sklearn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "def process_points(path):\n",
        "    points_df = pd.read_pickle(path)\n",
        "    points_df = gpd.GeoDataFrame(points_df, geometry=gpd.points_from_xy(np.float64(points_df.x), np.float64(points_df.y))) \n",
        "    return points_df\n",
        "\n",
        "\n",
        "def one_hot(points_df, column):\n",
        "    '''\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    points_df : points dataset\n",
        "        \n",
        "    column : string\n",
        "        a column of points_df in which you want to perform one hot encoding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    points_df : points dataset updated --> new columns are called is_nameColumns_categoricalName\n",
        "        \n",
        "\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    values = points_df[column]\n",
        "    # integer encode\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(values)\n",
        "    # binary encode\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "    \n",
        "\n",
        "    # now this information is added to the dataset, each column gets the name of the categorical type   \n",
        "    for i in range(onehot_encoded.shape[1]):\n",
        "        index = np.argwhere(integer_encoded == i)[0]\n",
        "        cat_type = values[index[0]]\n",
        "        names = 'is_' + column + '_' + str(cat_type)\n",
        "        points_df[names.split('.')[0]] = onehot_encoded[:,i]\n",
        "    \n",
        "    return points_df\n",
        "\n",
        "#check if the column of a dataset is an object and does the one hot encoding if so \n",
        "def one_hot_automatic(points_df):\n",
        "    \n",
        "    for i in points_df.columns:\n",
        "        type_ = points_df[i].dtype\n",
        "        if type_ == object:\n",
        "            print('I found \"', i, '\" as categorical then I am appling one hot encoding')\n",
        "            points_df = one_hot(points_df, i)\n",
        "            print('I am removing the categorical var from the dataset')\n",
        "            points_df.drop([i], axis = 1, inplace = True)\n",
        "    \n",
        "    return points_df\n",
        "\n",
        "def build_dataset(points_df, fires_df, sea):\n",
        "    '''\n",
        "    \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    points_df : df\n",
        "        points dataset.\n",
        "    fires_df : df\n",
        "        fires dataset.\n",
        "    sea : int\n",
        "        a number that identifies a season (1 = winter, 2 = summer).\n",
        "        this info is stored in the column 'season' of fires_df\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset_train : df\n",
        "        training dataset.\n",
        "    x_label : array\n",
        "        the labels related to the training set.\n",
        "    dataset_test : df\n",
        "        test dataset.\n",
        "    y_label : array\n",
        "        the labels related to the test set.\n",
        "\n",
        "    '''\n",
        "    \n",
        "    try:\n",
        "        fires_df['point_index'] = np.float64(fires_df.index)\n",
        "    except KeyError:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    season_fires_df = fires_df[fires_df['season'] == sea]\n",
        "    \n",
        "    # put 70% of burned point in training dataset\n",
        "    lenght_training_fires = int(len(season_fires_df) * 70/100)\n",
        "    \n",
        "    # at this point you can add a filter for sub sampling the training points --> reducing the number \n",
        "    # of burned points will reduce the number of not burned points having so a smaller training dataset.\n",
        "    \n",
        "    \n",
        "    fires_training =  season_fires_df.sample(n = lenght_training_fires)    \n",
        "    # fires_training = season_fires_df.loc[(season_fires_df['year'] >= year_from) & (season_fires_df['year'] < year_test)]\n",
        "    fire_points_train = fires_training.point_index \n",
        "    presences_train = points_df[points_df['point_index'].isin(fire_points_train)]\n",
        "    \n",
        "    absences_df = points_df[~points_df['point_index'].isin(season_fires_df.point_index)]\n",
        "    \n",
        "    # select number of non burned points equal to the number of fire points\n",
        "    sample_train = len(presences_train)\n",
        "    absence_train = absences_df.sample(n = sample_train)\n",
        "\n",
        "    # fires_testing = season_fires_df.loc[(season_fires_df['year'] >= year_test)]\n",
        "    fires_testing = season_fires_df[~season_fires_df['point_index'].isin(fires_training.point_index)]\n",
        "\n",
        "    presences_testing = points_df[points_df['point_index'].isin(fires_testing.point_index)]                                    \n",
        "    \n",
        "    #take test unburned data that are not even in the absence_train \n",
        "    sample_test = len(presences_testing)\n",
        "    absence_test = absences_df[~absences_df['point_index'].isin(absence_train.point_index)].sample(n = sample_test)\n",
        "    \n",
        "    # add the binary label --> dependent variable\n",
        "    pd.set_option('mode.chained_assignment', None)\n",
        "    presences_train['fires'] = 1\n",
        "    presences_testing['fires'] = 1\n",
        "\n",
        "    absence_train['fires'] = 0\n",
        "    absence_test['fires'] = 0\n",
        "    \n",
        "    # merge presences and absences                                \n",
        "    dataset_train = presences_train.append(absence_train)\n",
        "    dataset_test = presences_testing.append(absence_test)\n",
        " \n",
        "    print('lenght of training dataset ', len(dataset_train))\n",
        "    print('lenght of testing dataset ', len(dataset_test))\n",
        "\n",
        "    print('dataset train - label count ', np.unique(dataset_train.fires, return_counts = True))\n",
        "    print('dataset test - label count', np.unique(dataset_test.fires, return_counts = True))\n",
        "    \n",
        "\n",
        "    x_label = dataset_train.pop('fires')\n",
        "    y_label = dataset_test.pop('fires')\n",
        "    \n",
        "    return dataset_train, x_label, dataset_test, y_label\n",
        "\n",
        "def seasonal_clim_division(sea, dataset_train, dataset_test,\n",
        "                           names_clim_summer = ['temp_2', 'prec_2'], \n",
        "                           names_clim_winter = ['temp_1', 'prec_1']):\n",
        "    '''\n",
        "    \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sea : int\n",
        "        the season of the experiment, 1 if winter, 2 if summer.\n",
        "    dataset_train : df\n",
        "        trainig dataset.\n",
        "    dataset_test : df\n",
        "        test dataset.\n",
        "    names_clim_winter : list of objects\n",
        "        it contains the name of the climate varaibles only for the winter season (i.e ['T_mean_winter']).\n",
        "    names_clim_summer : list of objects\n",
        "        it contains the name of the climate varaibles only for the summer season (i.e ['T_mean_summer']).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset_train : df\n",
        "        the training dataset updated depending on the season of the experiment.\n",
        "    dataset_test : df\n",
        "        the test dataset updated depending on the season of the experiment.\n",
        "\n",
        "    '''\n",
        "    if names_clim_winter == None:\n",
        "        pass\n",
        "    else:\n",
        "        # in winter the columns referred to summer are dropped\n",
        "        if sea == 1:\n",
        "            dataset_train = dataset_train.drop(names_clim_summer, axis = 1)\n",
        "            dataset_test = dataset_test.drop(names_clim_summer, axis = 1)\n",
        "        # in summer the columns related to winter are dropped    \n",
        "        elif sea == 2:            \n",
        "            dataset_train = dataset_train.drop(names_clim_winter, axis = 1)\n",
        "            dataset_test = dataset_test.drop(names_clim_winter, axis = 1)\n",
        "    \n",
        "    return dataset_train, dataset_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_scores(model, y_test, y_label):\n",
        "    '''\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : sklearn object\n",
        "        selected ML algorithm.\n",
        "    y_test : df\n",
        "        test dataset.\n",
        "    y_label : array\n",
        "        labels of test dataset.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    auc : float\n",
        "        auc of the model.\n",
        "    cm : list\n",
        "        confusion matrix.\n",
        "    report : \n",
        "        some test scores and the general model's accuracy.\n",
        "    rmse : float\n",
        "        mean squared error.\n",
        "\n",
        "    '''\n",
        "    \n",
        "        \n",
        "    #the probability predicion\n",
        "    p_test = model.predict_proba(y_test)[:,1]\n",
        "    \n",
        "    #calculate score\n",
        "    auc = roc_auc_score(y_label, p_test)\n",
        "    \n",
        "    # no skill auc for plotting roc curve\n",
        "    ns_probs = [0 for _ in range(len(y_label))]\n",
        "    ns_auc = roc_auc_score(y_label, ns_probs)\n",
        "    \n",
        "    # calculate roc curves\n",
        "    ns_fpr, ns_tpr, _ = roc_curve(y_label, ns_probs)\n",
        "    fpr, tpr, _ = roc_curve(y_label, p_test)\n",
        "    # plot the roc curve for the model\n",
        "    plt.figure()\n",
        "    plt.plot(ns_fpr, ns_tpr, label='No Skill')\n",
        "    plt.plot(fpr, tpr, label='Classification model')\n",
        "    # axis labels\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC curve')\n",
        "    # show the legend\n",
        "    plt.legend()\n",
        "    \n",
        "    \n",
        "    \n",
        "    mse = mean_squared_error(y_label, p_test)\n",
        "    \n",
        "    names_ = ['no fire','fire']\n",
        "    \n",
        "    #the actulal 0 1 class prediction\n",
        "    p_test1 = model.predict(y_test)\n",
        "    \n",
        "    cm = confusion_matrix(y_label, p_test1, normalize = 'true')\n",
        "    report = classification_report(y_label, p_test1, target_names = names_) \n",
        "    \n",
        "    \n",
        "    plt.figure()\n",
        "    sns.heatmap(cm.T, annot = True, fmt = \".0%\", cmap = \"cividis\", xticklabels = names_, yticklabels = names_)\n",
        "    plt.xlabel(\"True label\")\n",
        "    plt.ylabel(\"Predicted label\")    \n",
        "    plt.title('confusion matrix')\n",
        "\n",
        "    print('roc_auc_test = ',auc)\n",
        "    print('report: \\n', report)\n",
        "    print('mse: ', mse)\n",
        "    \n",
        "    return auc, cm, report, mse\n",
        "\n",
        "\n",
        "# just in case a susc map tif format is wanted\n",
        "def to_raster(path_dem, p_burned, points_df, my_x , my_y, out_path):\n",
        "    '''\n",
        "    \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_dem : string\n",
        "        path of the dem file in tif or tiff format.\n",
        "    p_burned : array\n",
        "        array of probabilities for the label=1.\n",
        "    points_df : df\n",
        "        points set.\n",
        "    my_x : array\n",
        "        array of x coordinates.\n",
        "    my_y : TYPE\n",
        "        array of y coordinates.\n",
        "    out_path : string\n",
        "        path output file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    '''\n",
        "    \n",
        "    dem = rio.open(path_dem)\n",
        "    # d = dem.read()\n",
        "    \n",
        "    coordx = np.float64(my_x)\n",
        "    coordy = np.float64(my_y)\n",
        "\n",
        "\n",
        "    # bounds and resolution that I want for my raster (the dem one)\n",
        "    xmin = dem.bounds[0]\n",
        "    xmax = dem.bounds[2]\n",
        "    ymin = dem.bounds[1]\n",
        "    ymax = dem.bounds[3]\n",
        "    \n",
        "    xres = dem.transform[0]\n",
        "    yres = -dem.transform[4]\n",
        "\n",
        "    #create 2D array from 1D array of probabilities\n",
        "    statistic,x_edge,y_edge,binnumber = binned_statistic_2d(coordx, coordy, p_burned, 'mean',bins =[np.arange(xmin, xmax+xres, xres), np.arange(ymin, ymax+yres, yres)] )\n",
        "    \n",
        "    statistic = statistic.T\n",
        "    # np.shape(statistic)\n",
        "     \n",
        "    # the new meta for the new raster has to be equalt to the dem one     \n",
        "    new_meta = {'driver': 'GTiff',\n",
        "      'dtype': 'float64',\n",
        "      'nodata': dem.nodata,\n",
        "      'width': statistic.shape[1],\n",
        "      'height': statistic.shape[0],\n",
        "      'count': 1,\n",
        "      'crs': dem.crs,\n",
        "      'transform': (dem.transform[0], dem.transform[1], dem.transform[2], dem.transform[3], -dem.transform[4], dem.bounds[1] )}\n",
        "    \n",
        "    #prepare the right shape for exporting the matrix as tif file\n",
        "    statistics = np.zeros((1,statistic.shape[0], statistic.shape[1]))\n",
        "    statistics[0,:,:] = statistic\n",
        "    \n",
        "    # creating the map --> this has resolution avaraged when upload on qgis\n",
        "    folder_path = out_path.split('.')[0]\n",
        "    name_temporary_file = folder_path + '_temporary.tif'\n",
        "    with rio.open(name_temporary_file, 'w', **new_meta) as dest:\n",
        "        dest.write(statistics)\n",
        "    \n",
        "    # this 2 operations allow to visualize the right resolution on qgis\n",
        "    \n",
        "    gdal.Warp(out_path, name_temporary_file, xRes=xres, yRes=yres, geoloc = True)\n",
        "    gdal.Warp(out_path, name_temporary_file, xRes=xres, yRes=yres)\n",
        "    \n",
        "    print('I have created the susceptibility map')\n",
        "    \n",
        "    dest.close()\n",
        "    dem.close() \n",
        "    \n",
        "    os.remove(name_temporary_file)\n",
        "\n",
        "#%% input data\n",
        "\n",
        "vs = 'vs0'\n",
        "\n",
        "# input\n",
        "complete_df_path = r\"C:\\Users\\Giorg\\Dropbox\\CIMA\\tirocinio\\Liguria\\liguria\\ML_data\\ML_datasets\\{}\\points_liguria_vs0.1.pkl\".format(vs) \n",
        "fires_df_path    = r\"C:\\Users\\Giorg\\Dropbox\\CIMA\\tirocinio\\Liguria\\liguria\\ML_data\\ML_datasets\\{}\\fires_liguria_vs0.1.pkl\".format(vs) \n",
        "veg_path = r\"C:\\Users\\Giorg\\Dropbox\\CIMA\\tirocinio\\Liguria\\liguria\\ML_data\\veg\\{}\\raster_ML\\vegetation.tiff\".format(vs)\n",
        "\n",
        "\n",
        "# model params\n",
        "sea = 1          # seasonal analysis --> 1 = winter 2 = summer\n",
        "ntree = 10       # number of random forest estimators\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%%  model\n",
        "\n",
        "# load the datasets and apply one hot encoding whenever needed\n",
        "points_df = process_points(complete_df_path)\n",
        "points_df = one_hot_automatic(points_df) \n",
        "fires_df = process_points(fires_df_path)\n",
        "   \n",
        "# building dataset train and test \n",
        "x_train, x_label, y_test, y_label = build_dataset(points_df, fires_df, sea)\n",
        "\n",
        "# keep features related to the season of the analysis \n",
        "x_train, y_test = seasonal_clim_division(sea, x_train, y_test)\n",
        "\n",
        "# selection of feature to drop --> not used by the ML\n",
        "my_excluded_cols = ['row', 'col', 'x', 'y', 'point_index', 'geometry']\n",
        "columns =  [col for col in x_train if col not in my_excluded_cols]\n",
        "\n",
        "print('\\nfeatures of the model\\n')\n",
        "for i in columns:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "# remove not needed features\n",
        "x_train = x_train[columns]\n",
        "y_test = y_test[columns]\n",
        "\n",
        "# save coordinates for plotting the ssuc maps before dropping them \n",
        "my_x = points_df['x']\n",
        "my_y = points_df['y']\n",
        "\n",
        "\n",
        "points_df = points_df[columns]\n",
        "\n",
        "# inizialize the model \n",
        "model  = RandomForestClassifier(n_estimators = ntree, verbose = 2)\n",
        "# fit the model\n",
        "model.fit(x_train, x_label)\n",
        "# classification\n",
        "p = model.predict_proba(points_df)\n",
        "\n",
        "p_burned = p[:,1]\n",
        "\n",
        "# some performance indicators\n",
        "auc, cm, report, mse = test_scores(model, y_test, y_label)\n",
        "\n",
        "\n",
        "plt.scatter(my_x, my_y, c=p_burned)\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "my_theme = {\n",
        "              'legend.fontsize': 14,\n",
        "              'xtick.labelsize': 14,\n",
        "              'ytick.labelsize': 14,\n",
        "              'axes.labelsize': 16,\n",
        "              'axes.titlesize': 20,\n",
        "              }\n",
        "\n",
        "matplotlib.rcParams.update(my_theme)\n",
        "\n",
        "\n",
        "f, ax = plt.subplots(figsize= (10,6))\n",
        "\n",
        "dots = ax.scatter(my_x, my_y, c=p_burned, s=0.5)\n",
        "f.colorbar(dots)\n",
        "\n",
        "ax.set_xlabel('coord x')\n",
        "ax.set_ylabel('coord y')\n",
        "ax.set_title('Wildfire Susceptibility Map')\n",
        "\n",
        "\n",
        "#%% if you want to produce a susc map - tif file\n",
        "\n",
        "\n",
        "# for susc map\n",
        "from scipy.stats import binned_statistic_2d\n",
        "import os\n",
        "import gdal \n",
        "import rasterio as rio\n",
        "\n",
        "# more input\n",
        "dem_path = r\"C:\\Users\\Giorg\\Dropbox\\CIMA\\tirocinio\\Liguria\\liguria\\ML_data\\data_for_server\\vs0_marj_gurmej\\dtm_liguria_2017_100m_3003.tif\"\n",
        "\n",
        "# output \n",
        "output_susc_map = r\"C:\\Users\\Giorg\\Dropbox\\CIMA\\tirocinio\\Liguria\\liguria\\ML_data\\susc_maps\\{}\\susc_map_prova2.tif\".format(vs)  \n",
        "\n",
        "\n",
        "# produce susc map\n",
        "to_raster(dem_path, p_burned, points_df, my_x , my_y, output_susc_map)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%%\n"
      ]
    }
  ]
}